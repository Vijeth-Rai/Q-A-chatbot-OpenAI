{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will explore how to use Langchain with both OpenAI and Hugging Face models. \n",
    "\n",
    "Langchain is a powerful framework that allows us to interact with different language models and perform various natural language processing tasks. \n",
    "\n",
    "We will walk through the setup, integration, and use of these models step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPEN_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up OpenAI API Key\n",
    "\n",
    "We begin by setting up the environment variable for the OpenAI API key. This key is necessary to authenticate and access OpenAI's language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPEN_API_KEY\"]=OPEN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The provided API key is expired and will not work for actual requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the OpenAI Model\n",
    "\n",
    "Next, we initialize the OpenAI model with the API key and set a temperature for the model's responses. The temperature parameter controls the randomness of the output; lower values make the output more deterministic, while higher values make it more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction\n",
    "\n",
    "We then define a text prompt and use the model to make a prediction. The predict method sends the prompt to the model and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text=\"What is the capital of India\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Hugging Face Models\n",
    "\n",
    "In this section, we will set up the Hugging Face API token and initialize a Hugging Face model using Langchain. Hugging Face provides a wide range of pre-trained models for various natural language processing tasks.\n",
    "\n",
    "### Setting Up Hugging Face API Token\n",
    "\n",
    "First, we set up the environment variable for the Hugging Face API token. This token is required to authenticate and access Hugging Face's models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=HUGGINGFACEHUB_API_TOKEN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Hugging Face Model\n",
    "Next, we initialize a Hugging Face model using the HuggingFaceHub class from Langchain. We specify the model repository ID and set the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\New Recordings\\Langchain\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm_huggingface=HuggingFaceHub(repo_id=\"google/flan-t5-large\",model_kwargs={\"temperature\":0,\"max_length\":64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`repo_id`**: The ID of the model repository on Hugging Face. Here, we use \"google/flan-t5-large\".\n",
    "- **`model_kwargs`**: Additional parameters for the model. We set `temperature` to 0 for deterministic output and `max_length` to 64 to limit the length of the generated text.\n",
    "\n",
    "### Making Predictions with Hugging Face Model\n",
    "\n",
    "We can now make predictions using the Hugging Face model. We provide text prompts and use the `predict` method to get the model's responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moscow\n"
     ]
    }
   ],
   "source": [
    "output=llm_huggingface.predict(\"Can you tell me the capital of Russia\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the way i look at the world i love the way i feel i love the way i think i feel i love the way i feel i love the way i think i feel i love the way i feel i love the way \n"
     ]
    }
   ],
   "source": [
    "output=llm_huggingface.predict(\"Can you write a poem about AI\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates and LLMChain\n",
    "\n",
    "In this section, we will explore how to create prompt templates and use the `LLMChain` class from Langchain to process these templates with a language model.\n",
    "\n",
    "### Creating a Prompt Template\n",
    "\n",
    "A prompt template is a predefined structure that allows us to insert variables into a text prompt. This makes it easier to generate consistent and structured queries for the language model.\n",
    "\n",
    "#### Defining the Template\n",
    "\n",
    "We define a prompt template using the `PromptTemplate` class. This template includes a placeholder for the variable `country`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Tell me the capital of this {country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`input_variables`**: A list of variables that will be inserted into the template. Here, we have one variable, `country`.\n",
    "- **`template`**: The text template with placeholders for the variables. The placeholder `{country}` will be replaced with the actual country name.\n",
    "\n",
    "#### Formatting the Template\n",
    "\n",
    "We can format the template by providing the actual value for the variable. This generates the final prompt that will be sent to the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of this India'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(country=\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using LLMChain\n",
    "\n",
    "The `LLMChain` class allows us to chain the prompt template with the language model. This means we can feed the formatted prompt directly to the model and get the response in one step.\n",
    "\n",
    "#### Initializing LLMChain\n",
    "\n",
    "We create an instance of `LLMChain` by providing the language model (`llm`) and the prompt template (`prompt_template`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain=LLMChain(llm=llm,prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Chain\n",
    "\n",
    "To run the chain, we call the `run` method with the actual value for the variable. This method formats the template, sends the prompt to the model, and returns the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple Chains Using Simple Sequential Chain\n",
    "\n",
    "In this section, we will learn how to combine multiple language model chains using the `SimpleSequentialChain` class from Langchain. This allows us to process a sequence of tasks step-by-step, where the output of one task becomes the input for the next.\n",
    "\n",
    "### Creating Individual Chains\n",
    "\n",
    "#### Capital Chain\n",
    "\n",
    "First, we create a prompt template and chain to find the capital of a given country.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple Chains Uing simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm,prompt=capital_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`capital_template`**: A prompt template asking for the capital of a country.\n",
    "- **`capital_chain`**: An `LLMChain` that uses the `capital_template` to query the language model.\n",
    "\n",
    "#### Famous Places Chain\n",
    "\n",
    "Next, we create another prompt template and chain to suggest famous places to visit in a given capital city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "famous_chain=LLMChain(llm=llm,prompt=famous_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`famous_template`**: A prompt template asking for famous places in a capital city.\n",
    "- **`famous_chain`**: An `LLMChain` that uses the `famous_template` to query the language model.\n",
    "\n",
    "### Combining Chains with SimpleSequentialChain\n",
    "\n",
    "The `SimpleSequentialChain` class allows us to combine multiple chains in a sequence. The output of each chain is automatically passed as the input to the next chain.\n",
    "\n",
    "#### Initializing SimpleSequentialChain\n",
    "\n",
    "We create an instance of `SimpleSequentialChain` by providing a list of the individual chains we want to combine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain=SimpleSequentialChain(chains=[capital_chain,famous_chain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Combined Chain\n",
    "\n",
    "To run the combined chain, we call the `run` method with the initial input value. The chain will process this input through each individual chain in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" It is a bustling metropolis and a great place to visit for its historical sites, cultural attractions, and modern attractions. Here are some of the amazing places to visit in New Delhi: \\n\\n1. Red Fort: This 17th century Mughal fort is a UNESCO World Heritage Site and is one of the most popular tourist attractions in the city. \\n\\n2. India Gate: This iconic war memorial and national monument is a must-visit. It stands as a symbol of the sacrifice of the Indian soldiers who fought in World War I.\\n\\n3. Humayun's Tomb: This 16th century Mughal-era tomb is a UNESCO World Heritage Site and is one of the most important monuments in Delhi.\\n\\n4. Qutub Minar: This 73-meter-high tower is a UNESCO World Heritage Site and is one of the most iconic structures in Delhi.\\n\\n5. Jama Masjid: This 17th century mosque is one of the largest and most beautiful mosques in India.\\n\\n6. Lotus Temple: This modern temple is a Bahá'í House of Worship and is a popular tourist attraction.\\n\\n7. Akshardham Temple: This modern Hindu temple complex is a popular tourist destination\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SequentialChain for Advanced Workflows\n",
    "\n",
    "In this section, we will explore how to use the `SequentialChain` class from Langchain to handle more complex workflows. Unlike `SimpleSequentialChain`, `SequentialChain` allows us to manage multiple input and output variables, making it suitable for more intricate sequences of tasks.\n",
    "\n",
    "### Creating Prompt Templates and Chains\n",
    "\n",
    "#### Capital Chain with Output Key\n",
    "\n",
    "First, we create a prompt template and chain to find the capital of a given country. We specify an output key to store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm,prompt=capital_template,output_key=\"capital\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`capital_template`**: A prompt template asking for the capital of a country.\n",
    "- **`capital_chain`**: An `LLMChain` that uses the `capital_template` to query the language model and stores the result in the `capital` output key.\n",
    "\n",
    "#### Famous Places Chain with Output Key\n",
    "\n",
    "Next, we create another prompt template and chain to suggest famous places to visit in a given capital city. We also specify an output key for this chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "\n",
    "famous_chain=LLMChain(llm=llm,prompt=famous_template,output_key=\"places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`famous_template`**: A prompt template asking for famous places in a capital city.\n",
    "- **`famous_chain`**: An `LLMChain` that uses the `famous_template` to query the language model and stores the result in the `places` output key.\n",
    "\n",
    "### Combining Chains with SequentialChain\n",
    "\n",
    "The `SequentialChain` class allows us to combine multiple chains and manage multiple input and output variables. This makes it ideal for more advanced workflows.\n",
    "\n",
    "#### Initializing SequentialChain\n",
    "\n",
    "We create an instance of `SequentialChain` by providing a list of the individual chains, along with the input and output variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain=SequentialChain(chains=[capital_chain,famous_chain],\n",
    "input_variables=['country'],\n",
    "output_variables=['capital',\"places\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`chains`**: A list of chains to be executed in sequence.\n",
    "- **`input_variables`**: A list of input variables for the combined chain.\n",
    "- **`output_variables`**: A list of output variables to be returned by the combined chain.\n",
    "\n",
    "#### Running the Combined Chain\n",
    "\n",
    "To run the combined chain, we call the `__call__` method (using curly braces) with the initial input value. The chain processes this input through each individual chain in sequence, managing multiple inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'India',\n",
       " 'capital': '\\n\\nThe capital of India is New Delhi.',\n",
       " 'places': ' Here are some amazing places to visit in New Delhi: \\n\\n1. Red Fort: The majestic Red Fort is a 17th-century fort complex built by Mughal Emperor Shah Jahan. It is a UNESCO World Heritage Site and is a must-visit for all tourists. \\n\\n2. India Gate: India Gate is a 42 meter-high sandstone archway built by Edwin Lutyens in 1931. It is a memorial to the Indian soldiers who lost their lives during World War I. \\n\\n3. Qutub Minar: The Qutub Minar is a 73 meter-high tower built by Qutb-ud-din Aibak in 1193. It is a UNESCO World Heritage Site and is the tallest brick minaret in the world. \\n\\n4. Humayun’s Tomb: Humayun’s Tomb is a 16th-century tomb built by Mughal Emperor Humayun. It is a UNESCO World Heritage Site and is a great example of Mughal architecture. \\n\\n5. Jama Masjid: The Jama Masjid is a 17th-century mosque built by Mughal Emperor Shah Jahan. It is one of the largest'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'country':\"India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Chat Models with ChatOpenAI\n",
    "\n",
    "In this section, we will explore how to use chat models with the `ChatOpenAI` class from Langchain. Chat models are designed to handle interactive dialogues, making them ideal for creating conversational agents.\n",
    "\n",
    "### Importing Necessary Modules\n",
    "\n",
    "We start by importing the necessary modules for creating and managing chat messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`ChatOpenAI`**: This class allows us to interact with OpenAI's chat models.\n",
    "- **`HumanMessage`**: Represents a message from the human user.\n",
    "- **`SystemMessage`**: Represents a system message that can set the context or behavior for the AI.\n",
    "- **`AIMessage`**: Represents a message from the AI.\n",
    "\n",
    "### Initializing the Chat Model\n",
    "\n",
    "We initialize the `ChatOpenAI` model with the necessary parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm=ChatOpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6,model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`openai_api_key`**: The API key to access OpenAI's models.\n",
    "- **`temperature`**: Controls the randomness of the model's responses. A value of 0.6 adds some variability while maintaining coherence.\n",
    "- **`model`**: Specifies the model version to use. Here, we use 'gpt-3.5-turbo'.\n",
    "\n",
    "### Creating Chat Messages\n",
    "\n",
    "We create a sequence of messages to define the interaction between the user and the AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"AI may be smart, but can it tell me if my outfit makes me look like a potato?\"\\n2. \"AI is like a virtual therapist, except it never judges you for eating an entire pizza by yourself.\"\\n3. \"AI is great at predicting the future, but can it predict when my pizza delivery will actually arrive?\"\\n4. \"They say AI can learn from its mistakes, but I\\'m still waiting for it to apologize for recommending me that terrible movie.\"\\n5. \"AI may be able to beat humans at chess, but can it figure out how to untangle a pair of earphones?\"\\n6. \"AI is like a high-tech fortune teller, except it tells you what you\\'re going to have for dinner instead of your future.\"\\n7. \"AI is so advanced, it can even make my phone autocorrect my perfectly spelled words into complete nonsense.\"\\n8. \"AI may be able to recognize faces, but can it recognize when someone\\'s had a bad haircut?\"\\n9. \"AI is like having a personal assistant, except it never judges you for spending hours watching cat videos on YouTube.\"\\n10. \"AI is great at analyzing data, but can it analyze why I can never find matching socks in my drawer?\"')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "SystemMessage(content=\"Yor are a comedian AI assitant\"),\n",
    "HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`SystemMessage`**: Sets the initial context, telling the AI that it should act as a comedian assistant.\n",
    "- **`HumanMessage`**: Represents the user's input, asking for comedy punchlines related to AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Prompt Templates, Language Models, and Output Parsers\n",
    "\n",
    "In this section, we will combine prompt templates, language models, and custom output parsers to create a sophisticated text processing pipeline. This allows us to define structured prompts, interact with language models, and format the output in a specific way.\n",
    "\n",
    "### Importing Necessary Modules\n",
    "\n",
    "We start by importing the required modules for chat models, prompt templates, and output parsing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`ChatOpenAI`**: Class for interacting with OpenAI's chat models.\n",
    "- **`ChatPromptTemplate`**: Class for creating chat-specific prompt templates.\n",
    "- **`BaseOutputParser`**: Base class for custom output parsers.\n",
    "\n",
    "### Defining a Custom Output Parser\n",
    "\n",
    "We create a custom output parser by subclassing `BaseOutputParser`. This parser will split the text into a list of comma-separated values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`Commaseperatedoutput`**: Custom parser that splits the input text by commas and returns a list of values.\n",
    "\n",
    "### Creating Prompt Templates\n",
    "\n",
    "We define a prompt template using `ChatPromptTemplate`. This template includes a system message that sets the assistant's behavior and a human message template.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"Your are a helpful assistant. When the use given any input , you should generate 5 words synonyms in a comma seperated list\"\n",
    "human_template=\"{text}\"\n",
    "chatprompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`template`**: System message setting the context for the assistant.\n",
    "- **`human_template`**: Template for user input.\n",
    "\n",
    "### Combining Components into a Chain\n",
    "\n",
    "We combine the prompt template, language model, and output parser into a single processing chain using Langchain's pipeline syntax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=chatprompt|chatllm|Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`chatprompt`**: The prompt template defining the interaction structure.\n",
    "- **`chatllm`**: The language model for generating responses.\n",
    "- **`Commaseperatedoutput`**: The custom output parser for formatting the results.\n",
    "\n",
    "### Invoking the Chain\n",
    "\n",
    "We invoke the chain with an input text to get the processed output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' brilliant', ' sharp', ' astute']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By integrating prompt templates, language models, and custom output parsers, we can create flexible and powerful text processing workflows using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
